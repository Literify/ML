{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "TLrsVr9THWLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gG3jLOGbaUv"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-ranking\n",
        "!pip install -q tensorflow-recommenders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "PAEWhAvEcreJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZGYDaF-m5wZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import pprint\n",
        "import tempfile\n",
        "import shutil\n",
        "import kagglehub\n",
        "import pickle\n",
        "from typing import Dict, Text\n",
        "import ipywidgets as widgets\n",
        "import random\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import regex as re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ],
      "metadata": {
        "id": "UFlCIuf8IYsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxQ_hy7xPH3N"
      },
      "outputs": [],
      "source": [
        "import tensorflow_recommenders as tfrs\n",
        "import tensorflow_ranking as tfr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import pytesseract"
      ],
      "metadata": {
        "id": "3bzg8K9mc5ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCxQ1CZcO2wh"
      },
      "source": [
        "# Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mohamedbakhet_amazon_books_reviews_path = kagglehub.dataset_download('mohamedbakhet/amazon-books-reviews')\n",
        "print('Data source import complete')"
      ],
      "metadata": {
        "id": "hEiZts--z5oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make folder 'data'\n",
        "data_folder = 'data'\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "shutil.move(mohamedbakhet_amazon_books_reviews_path, os.path.join(data_folder, 'amazon-books-reviews'))\n",
        "print(f'Dataset has been moved to folder {data_folder}')"
      ],
      "metadata": {
        "id": "k2c7tq2fdCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_file_path = 'data/amazon-books-reviews/Books_rating.csv'\n",
        "books_details_file_path = 'data/amazon-books-reviews/books_data.csv'\n",
        "\n",
        "# Load the reviews file\n",
        "reviews_df = pd.read_csv(reviews_file_path)\n",
        "\n",
        "# Load the books details file\n",
        "books_details_df = pd.read_csv(books_details_file_path)"
      ],
      "metadata": {
        "id": "oCb7fTTyd8Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the reviews dataset\n",
        "print(\"Reviews Dataset:\")\n",
        "print(reviews_df.info())\n",
        "\n",
        "# Explore the books details dataset\n",
        "print(\"\\nBooks Details Dataset:\")\n",
        "print(books_details_df.info())"
      ],
      "metadata": {
        "id": "ZtbGFaTdeK0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Dataset"
      ],
      "metadata": {
        "id": "E4Cb_v7WegLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in reviews dataset\n",
        "reviews_missing = reviews_df.isnull().sum()\n",
        "\n",
        "# Check for missing values in books details dataset\n",
        "books_missing = books_details_df.isnull().sum()\n",
        "\n",
        "# Display missing values\n",
        "print(\"Reviews Missing Values:\")\n",
        "print(reviews_missing)\n",
        "\n",
        "print(\"\\nBooks Details Missing Values:\")\n",
        "print(books_missing)"
      ],
      "metadata": {
        "id": "7IdHwdISe9Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing 'Title' and 'User_id'\n",
        "reviews_df = reviews_df.dropna(subset=['Title', 'User_id'])\n",
        "\n",
        "# Drop column that we won't be using it\n",
        "reviews_df = reviews_df.drop(columns=['profileName'])\n",
        "\n",
        "# Fill missing values in 'review/summary' and 'review/text' with empty strings\n",
        "reviews_df['review/summary'] = reviews_df['review/summary'].fillna('')\n",
        "reviews_df['review/text'] = reviews_df['review/text'].fillna('')\n",
        "\n",
        "# Display updated information about missing values\n",
        "reviews_missing_values = reviews_df.isnull().sum()\n",
        "print(\"Reviews Missing Values After Handling:\")\n",
        "print(reviews_missing_values)"
      ],
      "metadata": {
        "id": "52RfDVNWfGxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing 'Title'\n",
        "books_details_df = books_details_df.dropna(subset=['Title'])\n",
        "\n",
        "# Impute missing values in 'ratingsCount' with the median\n",
        "books_details_df['ratingsCount'] = books_details_df['ratingsCount'].fillna(books_details_df['ratingsCount'].median())\n",
        "\n",
        "# Fill missing values in textual columns with empty strings\n",
        "textual_columns = ['description', 'authors', 'publisher', 'publishedDate', 'categories']\n",
        "books_details_df[textual_columns] = books_details_df[textual_columns].fillna('')\n",
        "\n",
        "# Dropping columns we are not going to use\n",
        "# books_details_df = books_details_df.drop(columns=['image', 'previewLink', 'infoLink'])\n",
        "\n",
        "# Display updated information about missing values\n",
        "books_details_missing_values = books_details_df.isnull().sum()\n",
        "print(\"Books Details Missing Values After Handling:\")\n",
        "print(books_details_missing_values)"
      ],
      "metadata": {
        "id": "xXVUdbNDf0IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_genre_column(df, column_name):\n",
        "    def process_genre(genre):\n",
        "        # Remove square brackets and quotes\n",
        "        cleaned_genre = genre.strip(\"[]\").replace(\"'\", \"\").strip()\n",
        "        # Replace '&' with ',' and split by ','\n",
        "        # genres = cleaned_genre.replace('&', ',').split(',')\n",
        "        # Strip whitespace, convert to lowercase, and sort the genres alphabetically\n",
        "        # sorted_genres = sorted(g.strip().lower() for g in genres)\n",
        "        # Join back with ', '\n",
        "        # return ', '.join(sorted_genres)\n",
        "        return cleaned_genre\n",
        "    # Apply the processing function to the specified column\n",
        "    df[column_name] = df[column_name].apply(process_genre)\n",
        "    return df"
      ],
      "metadata": {
        "id": "6UKZNhSrsOz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books_details_df = preprocess_genre_column(books_details_df, 'categories')"
      ],
      "metadata": {
        "id": "OXEGpw0msjwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books_details_df = preprocess_genre_column(books_details_df, 'authors')"
      ],
      "metadata": {
        "id": "W4IGBEgrxN8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df.head()"
      ],
      "metadata": {
        "id": "tniw2QthuV16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books_details_df.head()"
      ],
      "metadata": {
        "id": "2w1KPBNruQKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(reviews_df, books_details_df, on='Title', how='left')\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "_HNgGMd85u7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(merged_df)"
      ],
      "metadata": {
        "id": "efUM35VPl82q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop_duplicates()"
      ],
      "metadata": {
        "id": "q2ogQ4dImYsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(merged_df)"
      ],
      "metadata": {
        "id": "cdaOlucJmezQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_missing_values = merged_df.isnull().sum()\n",
        "print(\"Merged Dataframe Missing Values:\")\n",
        "print(merged_df_missing_values)"
      ],
      "metadata": {
        "id": "gnp1e6Vi0NDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling Dataset"
      ],
      "metadata": {
        "id": "wCAlRwRpOEpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 10 most frequent genres in the 'categories' column\n",
        "top_10_frequent_genres = merged_df['categories'].value_counts().head(11)\n",
        "\n",
        "print(\"Top 10 most frequent genres in the 'genre' column:\")\n",
        "print(top_10_frequent_genres)"
      ],
      "metadata": {
        "id": "17G64BViOJ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "    'Fiction',\n",
        "    'Juvenile Fiction',\n",
        "    'Biography & Autobiography',\n",
        "    'Religion',\n",
        "    'History',\n",
        "    'Business & Economics',\n",
        "    'Computers',\n",
        "    'Social Science',\n",
        "    'Cooking',\n",
        "    'Self-Help'\n",
        "]\n",
        "\n",
        "# First, filter merged_df to exclude rows where 'image', 'previewLink', or 'infoLink' are NaN\n",
        "filtered_df = merged_df.dropna(subset=['image', 'previewLink', 'infoLink', 'Price'])\n",
        "\n",
        "# Then, filter further to only include rows where the 'categories' column is in the specified categories list\n",
        "filtered_df = filtered_df[filtered_df['categories'].isin(categories)]\n",
        "\n",
        "# Sample 5000 rows per category\n",
        "sampled_df = filtered_df.groupby('categories').apply(lambda x: x.sample(n=5000, random_state=42))\n",
        "\n",
        "# Reset the index after sampling\n",
        "sampled_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "8inISLAXPbqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result\n",
        "sampled_df['categories'].value_counts()"
      ],
      "metadata": {
        "id": "zY5TAanoidKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df.head()"
      ],
      "metadata": {
        "id": "QqvpL-woiiwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df.info()"
      ],
      "metadata": {
        "id": "vBXdFAIYim_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df.to_csv('sampled_categories.csv', index=False)"
      ],
      "metadata": {
        "id": "Qc0JPLQ0P4O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capstone_path = kagglehub.dataset_download(\"dikiiwahyudi/capstone-literify\")\n",
        "print('Data source import complete')"
      ],
      "metadata": {
        "id": "SNRymJzr2B_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make folder 'data'\n",
        "data_folder = 'data'\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "shutil.move(capstone_path, os.path.join(data_folder, 'full-data'))\n",
        "print(f'Dataset has been moved to folder {data_folder}')"
      ],
      "metadata": {
        "id": "DJPGNBZV2B_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/data/full-data/sampled_categories.csv\", on_bad_lines='skip')"
      ],
      "metadata": {
        "id": "Zk22niADtiGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "7WtdAUGetrx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "bDnG9KxY5R1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['categories'].value_counts()"
      ],
      "metadata": {
        "id": "JamQAwESg3Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = df[df.duplicated()]\n",
        "print(duplicate_rows)"
      ],
      "metadata": {
        "id": "ZSB9RBVaoO3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1: Without Genre"
      ],
      "metadata": {
        "id": "JavxMaZsphYi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PAqjR4a1RR4"
      },
      "source": [
        "## Preparing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'review/score': 'user_rating', 'User_id': 'user_id', 'Title': 'book_title'})"
      ],
      "metadata": {
        "id": "d1M9tBbP_o16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['user_id', 'book_title', 'user_rating']].head()"
      ],
      "metadata": {
        "id": "3HqKWa5uCWZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_books_df = df[['book_title']].drop_duplicates()\n",
        "unique_books_df.head()"
      ],
      "metadata": {
        "id": "vb_yagUE4Uz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = tf.data.Dataset.from_tensor_slices(dict(df[['user_id', 'book_title', 'user_rating']]))\n",
        "books = tf.data.Dataset.from_tensor_slices(dict(unique_books_df[['book_title']]))\n",
        "\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"book_title\": x[\"book_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"rating\": float(x[\"user_rating\"])\n",
        "})\n",
        "\n",
        "books = books.map(lambda x: x[\"book_title\"])"
      ],
      "metadata": {
        "id": "zFLZwnYqDKfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# Calculate the total number of elements in the dataset\n",
        "total_count = len(ratings)\n",
        "\n",
        "# Determine the size of the training and testing datasets\n",
        "train_size = int(0.8 * total_count)\n",
        "test_size = total_count - train_size\n",
        "\n",
        "# Shuffle the dataset\n",
        "shuffled = ratings.shuffle(total_count, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "# Split the dataset into 80% training and 20% testing\n",
        "train = shuffled.take(train_size)\n",
        "test = shuffled.skip(train_size).take(test_size)"
      ],
      "metadata": {
        "id": "ruDQ42xHEVC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_titles = books.batch(1_000)\n",
        "user_ids = ratings.batch(1_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_book_titles = np.unique(np.concatenate(list(book_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
        "\n",
        "print('Unique books: {}'.format(len(unique_book_titles)))\n",
        "print('Unique users: {}'.format(len(unique_user_ids)))"
      ],
      "metadata": {
        "id": "Ttp32uCfFpvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture"
      ],
      "metadata": {
        "id": "DayUZzMDp0wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BookModel(tfrs.models.Model):\n",
        "    def __init__(self, rating_weight: float, retrieval_weight: float) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        embedding_dimension = 32\n",
        "\n",
        "        # User and book models with smaller embeddings\n",
        "        self.book_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_book_titles, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_book_titles) + 1, embedding_dimension)\n",
        "        ])\n",
        "        self.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "        ])\n",
        "\n",
        "        # Rating model\n",
        "        self.rating_model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(1),\n",
        "        ])\n",
        "\n",
        "        # Ranking task\n",
        "        self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
        "            loss=tf.keras.losses.MeanSquaredError(),\n",
        "            metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        "        )\n",
        "        # Retrieval task\n",
        "        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
        "            metrics=tfrs.metrics.FactorizedTopK(\n",
        "                candidates=books.batch(248).map(self.book_model)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Loss weights\n",
        "        self.rating_weight = rating_weight\n",
        "        self.retrieval_weight = retrieval_weight\n",
        "\n",
        "    def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "        user_embeddings = self.user_model(features[\"user_id\"])\n",
        "        book_embeddings = self.book_model(features[\"book_title\"])\n",
        "\n",
        "        return (\n",
        "            user_embeddings,\n",
        "            book_embeddings,\n",
        "            self.rating_model(\n",
        "                tf.concat([user_embeddings, book_embeddings], axis=1)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "        ratings = features.pop(\"rating\")\n",
        "        user_embeddings, book_embeddings, rating_predictions = self(features)\n",
        "\n",
        "        # Compute loss for each task\n",
        "        rating_loss = self.rating_task(labels=ratings, predictions=rating_predictions)\n",
        "        retrieval_loss = self.retrieval_task(user_embeddings, book_embeddings)\n",
        "\n",
        "        # Combine losses using weights\n",
        "        return (self.rating_weight * rating_loss + self.retrieval_weight * retrieval_loss)"
      ],
      "metadata": {
        "id": "eHnrGu3RGOeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting and Evaluating"
      ],
      "metadata": {
        "id": "rLUW-X7xp8Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model\n",
        "model = BookModel(rating_weight=1.0, retrieval_weight=1.0)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))"
      ],
      "metadata": {
        "id": "9XSiAHxKPIb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cached_train = train.shuffle(1_000).batch(248).cache()\n",
        "cached_test = test.batch(248).cache()"
      ],
      "metadata": {
        "id": "tOx-vzG1HkV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"root_mean_squared_error\",\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "2ix0rQh6PSph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(cached_train, validation_data=cached_test, epochs=10, callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "741QTbBxPWtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"\\nRetrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}\")\n",
        "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}\")"
      ],
      "metadata": {
        "id": "xlPUnqaJMArI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "BdRsWQKyqDob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('model_recomendation_weights.h5')"
      ],
      "metadata": {
        "id": "YjHeR-svGeuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_user_ids[:5]"
      ],
      "metadata": {
        "id": "IJikVUS974Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('user_ids.csv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['user_id']) # Write the header\n",
        "    for user_id in unique_user_ids:\n",
        "        writer.writerow([user_id.decode('utf-8')]) # Decode the byte string to a regular string and write"
      ],
      "metadata": {
        "id": "pvtPwgBe8ISa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "JzRa_B7jqHRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/content/model_recomendation_weights.h5\")"
      ],
      "metadata": {
        "id": "XvLpxrJJ7EiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_df = pd.read_csv('/content/content_df.csv')"
      ],
      "metadata": {
        "id": "Hw62ANYL7gFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_df.head()"
      ],
      "metadata": {
        "id": "dNbajJOiA4Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids_df = pd.read_csv('/content/user_ids.csv')"
      ],
      "metadata": {
        "id": "P-E_aWEY8TgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids_df.head()"
      ],
      "metadata": {
        "id": "8mom0lq_A5jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display catalog-style recommendations\n",
        "def display_catalog(recommendations, top_n=3):\n",
        "    display_str =  f\"<h3>Top {top_n} Recommendations:</h3><br>\"\n",
        "\n",
        "    for idx, row in recommendations.iterrows():\n",
        "        display_str += f\"\"\"\n",
        "            <div style=\"border: 1px solid #ddd; padding: 10px; margin-bottom: 10px;\">\n",
        "                <h4>{row['book_title']}</h4>\n",
        "                <p><strong>Authors:</strong> {row['authors']}</p>\n",
        "                <p><strong>Genre:</strong> {row['genre']}</p>\n",
        "                <p><strong>Publisher:</strong> {row['publisher']}</p>\n",
        "                <p><strong>Price:</strong> {row['Price']}</p>\n",
        "                <p><strong>Description:</strong> {row['description']}</p>\n",
        "                <img src=\"{row['image']}\" alt=\"{row['book_title']}\" width=\"100\" height=\"150\" style=\"display:block; margin-top: 10px;\">\n",
        "                <a href=\"{row['previewLink']}\" target=\"_blank\">Preview</a> |\n",
        "                <a href=\"{row['infoLink']}\" target=\"_blank\">More Info</a>\n",
        "            </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Display the catalog-style information\n",
        "    display(HTML(display_str))"
      ],
      "metadata": {
        "id": "8cOiY2qzDctk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_book_recomendation(user, filtered_books_df, top_n=3):\n",
        "    books = tf.data.Dataset.from_tensor_slices(dict(filtered_books_df[['book_title']]))\n",
        "    books = books.map(lambda x: x[\"book_title\"])\n",
        "\n",
        "    # Create a model that takes in raw query features\n",
        "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "\n",
        "    # Recommends books out of the entire books dataset\n",
        "    index.index_from_dataset(\n",
        "        tf.data.Dataset.zip((books.batch(100), books.batch(100).map(model.book_model)))\n",
        "    )\n",
        "\n",
        "    # Get recommendations\n",
        "    _, titles = index(tf.constant([str(user)]))\n",
        "    recommended_titles = [title.decode(\"utf-8\") for title in titles[0, :top_n].numpy()]\n",
        "\n",
        "    # Filter details from the input DataFrame\n",
        "    recommendations = filtered_books_df[filtered_books_df['book_title'].isin(recommended_titles)]\n",
        "    return display_catalog(recommendations, top_n=3)\n",
        "def predict_rating(user, book):\n",
        "    trained_book_embeddings, trained_user_embeddings, predicted_rating = model({\n",
        "          \"user_id\": np.array([str(user)]),\n",
        "          \"book_title\": np.array([book])\n",
        "      })\n",
        "    print(\"Predicted rating for {}: {}\".format(book, predicted_rating.numpy()[0][0]))"
      ],
      "metadata": {
        "id": "YoWWoEdiDTA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_book_recomendation(\"AA\", content_df, 5)"
      ],
      "metadata": {
        "id": "eDkPY1b2DZjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_rating(\"123\",\"Minions\")"
      ],
      "metadata": {
        "id": "mKAg1U86Dn4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_book(user_ids, content_df, model, predict_genre, top_n=3):\n",
        "    # Randomly sample a user_id from the user_ids array\n",
        "    user = random.choice(user_ids['user_id'].tolist())  # Get a random user_id from the list\n",
        "\n",
        "    # Ask the user if they want recommendations based on a predicted genre\n",
        "    genre_based = input(\"Would you like to see recommendations based on a predicted genre? (yes/no): \").strip().lower()\n",
        "\n",
        "    if genre_based == \"yes\":\n",
        "        # Filter books based on the selected genre\n",
        "        filtered_books_df = content_df[content_df['genre'].isin(predict_genre)].drop_duplicates().reset_index(drop=True)\n",
        "        recommendations = predict_book_recomendation(user, filtered_books_df, top_n)\n",
        "    else:\n",
        "        # If user doesn't want genre-based, use the entire dataset and remove duplicates\n",
        "        filtered_books_df = content_df.drop_duplicates().reset_index(drop=True)\n",
        "        recommendations = predict_book_recomendation(user, filtered_books_df, top_n)"
      ],
      "metadata": {
        "id": "vK1CUdGHDxG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "predict_genre = [\"Fiction\"]\n",
        "predict_book(user_ids_df, content_df, model, predict_genre, top_n=3)"
      ],
      "metadata": {
        "id": "-7JQgYJqD7JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2: With Genre"
      ],
      "metadata": {
        "id": "3lhLUOLkqW61"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSiyVy0FqW62"
      },
      "source": [
        "## Preparing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.rename(columns={'review/score': 'user_rating', 'User_id': 'user_id', 'Title': 'book_title', 'categories': 'genre'})"
      ],
      "metadata": {
        "id": "xQxjOGpyqW63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df[['user_id', 'book_title', 'user_rating', 'genre']].head()"
      ],
      "metadata": {
        "id": "w6a73D2TqW65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_genre_column(df, column_name):\n",
        "    def process_genre(genre):\n",
        "        # Remove square brackets and quotes\n",
        "        cleaned_genre = genre.strip(\"[]\").replace(\"'\", \"\").strip()\n",
        "        # Replace '&' with ',' and split by ','\n",
        "        genres = cleaned_genre.replace('&', ',').split(',')\n",
        "        # Strip whitespace, convert to lowercase, and sort the genres alphabetically\n",
        "        sorted_genres = sorted(g.strip().lower() for g in genres)\n",
        "        # Join back with ', '\n",
        "        return ', '.join(sorted_genres)\n",
        "\n",
        "    # Apply the processing function to the specified column\n",
        "    df[column_name] = df[column_name].apply(process_genre)\n",
        "    return df"
      ],
      "metadata": {
        "id": "RbBza_Dutap-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = preprocess_genre_column(merged_df, 'genre')"
      ],
      "metadata": {
        "id": "G2fDIH6Etq-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['genre'].head()"
      ],
      "metadata": {
        "id": "pMfGrWwWt0F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['genre'].nunique()"
      ],
      "metadata": {
        "id": "NZdD6aO-rGAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_model = pd.DataFrame(merged_df, columns=['user_id', 'book_title', 'genre', 'user_rating'])"
      ],
      "metadata": {
        "id": "bQL1ne6yqW66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets (80/20 split)\n",
        "train_df, test_df = train_test_split(merged_df_model, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "RiUspqb7qW66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df), len(test_df)"
      ],
      "metadata": {
        "id": "wQIQRmMGqW67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_model(df):\n",
        "  # Prepare the input data for training and testing\n",
        "  df_user_ids = df['user_id'].values\n",
        "  df_book_titles = df['book_title'].values\n",
        "  df_genres = df['genre'].values\n",
        "  df_ratings = df['user_rating'].values\n",
        "  return df_user_ids, df_book_titles, df_genres, df_ratings"
      ],
      "metadata": {
        "id": "yqBd12xqqW68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_user_ids, train_book_titles, train_genres, train_ratings = input_model(train_df)\n",
        "test_user_ids, test_book_titles, test_genres, test_ratings = input_model(test_df)"
      ],
      "metadata": {
        "id": "CNolXIS2qW68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_user_ids), len(train_book_titles), len(train_genres), len(train_ratings)"
      ],
      "metadata": {
        "id": "x63Cp8wsqW69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_user_ids), len(test_book_titles), len(test_genres), len(test_ratings)"
      ],
      "metadata": {
        "id": "FWS8EK6uqW6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary"
      ],
      "metadata": {
        "id": "Orq2lsnCqW7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_vocab = tf.keras.layers.StringLookup(vocabulary=merged_df_model['user_id'].astype(str).unique(), mask_token=None)\n",
        "book_vocab = tf.keras.layers.StringLookup(vocabulary=merged_df_model['book_title'].astype(str).unique(), mask_token=None)\n",
        "genre_vocab = tf.keras.layers.StringLookup(vocabulary=merged_df_model['genre'].astype(str).unique(), mask_token=None)"
      ],
      "metadata": {
        "id": "AI8seChZqW7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vocabulary from StringLookup\n",
        "user_vocab_list = user_vocab.get_vocabulary()\n",
        "book_vocab_list = book_vocab.get_vocabulary()\n",
        "genre_vocab_list = genre_vocab.get_vocabulary()\n",
        "\n",
        "# Save the vocabulary to .pkl files\n",
        "with open(\"user_vocab.pkl\", \"wb\") as uv_file:\n",
        "    pickle.dump(user_vocab_list, uv_file)\n",
        "\n",
        "with open(\"book_vocab.pkl\", \"wb\") as bv_file:\n",
        "    pickle.dump(book_vocab_list, bv_file)\n",
        "\n",
        "with open(\"genre_vocab.pkl\", \"wb\") as gv_file:\n",
        "    pickle.dump(genre_vocab_list, gv_file)"
      ],
      "metadata": {
        "id": "C-RCGe5BqW7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_user_ids = user_vocab(train_user_ids)\n",
        "train_book_titles = book_vocab(train_book_titles)\n",
        "train_genres = genre_vocab(train_genres)\n",
        "test_user_ids = user_vocab(test_user_ids)\n",
        "test_book_titles = book_vocab(test_book_titles)\n",
        "test_genres = genre_vocab(test_genres)"
      ],
      "metadata": {
        "id": "pjsq5u81qW7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture"
      ],
      "metadata": {
        "id": "1CcMfi5tqW7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hybrid collaborative filtering + content-based model\n",
        "def create_hybrid_model(user_vocab, movie_vocab, genre_vocab, embedding_dim=64):\n",
        "    # User input (Collaborative Filtering part)\n",
        "    user_input = Input(shape=(1,))\n",
        "    user_embedding = Embedding(user_vocab.vocabulary_size() + 1, embedding_dim)(user_input)\n",
        "    user_embedding = Flatten()(user_embedding)\n",
        "\n",
        "    # Book input (Collaborative Filtering part)\n",
        "    book_input = Input(shape=(1,))\n",
        "    book_embedding = Embedding(book_vocab.vocabulary_size() + 1, embedding_dim)(book_input)\n",
        "    book_embedding = Flatten()(book_embedding)\n",
        "\n",
        "    # Genre input (Content-based Filtering part)\n",
        "    genre_input = Input(shape=(1,))\n",
        "    genre_embedding = Embedding(genre_vocab.vocabulary_size() + 1, embedding_dim)(genre_input)\n",
        "    genre_embedding = Flatten()(genre_embedding)\n",
        "\n",
        "    # Combine collaborative filtering and content-based filtering\n",
        "    combined = Concatenate()([user_embedding, book_embedding, genre_embedding])\n",
        "\n",
        "    # Fully connected layer for prediction\n",
        "    dense_layer = Dense(128, activation='relu')(combined)\n",
        "    output = Dense(1)(dense_layer)\n",
        "\n",
        "    model = Model(inputs=[user_input, book_input, genre_input], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8E0Znd5xqW7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting and Evaluating"
      ],
      "metadata": {
        "id": "598w6RcIqW7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the hybrid model\n",
        "model = create_hybrid_model(user_vocab, book_vocab, genre_vocab, embedding_dim=64)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "E4EHiL5bqW7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    [train_user_ids, train_book_titles, train_genres], train_ratings,\n",
        "    epochs=3, batch_size=2048,\n",
        "    validation_data=([test_user_ids, test_book_titles, test_genres], test_ratings)\n",
        "    )"
      ],
      "metadata": {
        "id": "lJLG64CCqW7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "HvExbqt2qW7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('recommender_model.keras') # Save the model weights\n",
        "print(\"Model weights saved successfully!\")"
      ],
      "metadata": {
        "id": "U5e20R03qW7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save only the model weights\n",
        "model.save_weights('hybrid_model_weights.h5')"
      ],
      "metadata": {
        "id": "DB_idEifuazQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/content/hybrid_model_weights.h5\")"
      ],
      "metadata": {
        "id": "EuQCL_gixo8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "IzuCa3u3qW7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_resources(user_vocab_path, book_vocab_path, genre_vocab_path):\n",
        "    \"\"\"\n",
        "    Load the recommender model and vocabularies from saved files.\n",
        "\n",
        "    Parameters:\n",
        "    - model_path: Path to the saved Keras model (.h5 file).\n",
        "    - user_vocab_path: Path to the user vocabulary pickle file (.pkl).\n",
        "    - book_vocab_path: Path to the book vocabulary pickle file (.pkl).\n",
        "    - genre_vocab_path: Path to the genre vocabulary pickle file (.pkl).\n",
        "\n",
        "    Returns:\n",
        "    - model: Loaded Keras model.\n",
        "    - user_vocab, book_vocab, genre_vocab: Loaded vocabularies.\n",
        "    \"\"\"\n",
        "    # Load the trained Keras model\n",
        "    #model = tf.saved_model.load(model_path)\n",
        "\n",
        "    # Load the vocabularies\n",
        "    with open(user_vocab_path, 'rb') as file:\n",
        "        user_vocab = pickle.load(file)\n",
        "    with open(book_vocab_path, 'rb') as file:\n",
        "        book_vocab = pickle.load(file)\n",
        "    with open(genre_vocab_path, 'rb') as file:\n",
        "        genre_vocab = pickle.load(file)\n",
        "\n",
        "    return user_vocab, book_vocab, genre_vocab"
      ],
      "metadata": {
        "id": "MSUU2zn4dWjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to saved files\n",
        "user_vocab_path = \"user_vocab.pkl\"\n",
        "book_vocab_path = \"book_vocab.pkl\"\n",
        "genre_vocab_path = \"genre_vocab.pkl\"\n",
        "\n",
        "# Load resources\n",
        "user_vocab, book_vocab, genre_vocab = load_resources(user_vocab_path, book_vocab_path, genre_vocab_path)"
      ],
      "metadata": {
        "id": "l4GSfA7vdsb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books = tf.data.Dataset.from_tensor_slices(dict(merged_df[['book_title']]))"
      ],
      "metadata": {
        "id": "U17B1Iqhya2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books = books.map(lambda x: {\n",
        "    \"book_title\": x[\"book_title\"]\n",
        "})"
      ],
      "metadata": {
        "id": "CI89EeWAyrGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(user_vocab))\n",
        "print(type(book_vocab))\n",
        "print(type(genre_vocab))"
      ],
      "metadata": {
        "id": "OtgHTmeJ0OIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Safe lookup function for lists\n",
        "def safe_lookup(vocab_list, value, default=0):\n",
        "    try:\n",
        "        return vocab_list.index(value)  # Get index if found in list\n",
        "    except ValueError:\n",
        "        return default  # Return default value (0) if not found\n",
        "\n",
        "# Example user, genre, and book title data\n",
        "user_ids = [\"42\"]  # List of user IDs\n",
        "genres = [\"['religion']\"]  # List of genres\n",
        "book_titles = [\"The Bible\", \"Introduction to Programming\", \"The Catcher in the Rye\"]  # List of book titles\n",
        "\n",
        "# Encode the inputs using the vocabularies\n",
        "encoded_user_id = [safe_lookup(user_vocab, user) for user in user_ids]\n",
        "encoded_genre = [safe_lookup(genre_vocab, genre) for genre in genres]\n",
        "encoded_book_titles = [safe_lookup(book_vocab, title) for title in book_titles]\n",
        "\n",
        "# Convert them to tensors\n",
        "inputs = {\n",
        "    'input_1': tf.convert_to_tensor(encoded_user_id),  # user_id (numerical)\n",
        "    'input_2': tf.convert_to_tensor(encoded_genre),    # genre (numerical)\n",
        "    'input_3': tf.convert_to_tensor(encoded_book_titles)  # book_title (numerical)\n",
        "}\n",
        "\n",
        "# Get book recommendations for user 42\n",
        "scores = model(inputs)\n",
        "titles = tfr.utils.sort_by_scores(scores, [tf.convert_to_tensor(book_titles)])[0]\n",
        "print(f\"Top 5 recommendations for user 42: {titles[0, :5]}\")"
      ],
      "metadata": {
        "id": "4OiMqpjjx6ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_layer in model.inputs:\n",
        "    print(input_layer.name)"
      ],
      "metadata": {
        "id": "SBbr6GqQzgIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_book_recommendations(model, genre_vocab, books_df, user_id, user_genre_preferences, batch_size=2000):\n",
        "    \"\"\"\n",
        "    Generate book recommendations for a specific user based on their genre preferences.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained recommendation model.\n",
        "    - genre_vocab: Genre vocabulary to map genres to integer IDs.\n",
        "    - books_df: DataFrame containing the book dataset.\n",
        "    - user_id: ID of the user (e.g., 42).\n",
        "    - user_genre_preferences: List of genres the user likes (e.g., ['Fiction', 'Science Fiction']).\n",
        "    - batch_size: Number of books to process per batch (default: 2000).\n",
        "\n",
        "    Returns:\n",
        "    - top_recommendations: The top 5 book recommendations for the user.\n",
        "    \"\"\"\n",
        "    # Convert genre preferences to tensor using genre_vocab\n",
        "    genre_ids = [genre_vocab.get(genre, -1) for genre in user_genre_preferences]\n",
        "    genre_tensor = tf.convert_to_tensor(genre_ids, dtype=tf.int32)\n",
        "\n",
        "    # Create a TensorFlow dataset of book titles for batching\n",
        "    book_titles = books_df['Title'].values\n",
        "    book_titles = tf.data.Dataset.from_tensor_slices(book_titles)\n",
        "\n",
        "    # Batch the dataset to process in chunks\n",
        "    book_titles = book_titles.batch(batch_size)\n",
        "\n",
        "    # Initialize a list to store all recommendations\n",
        "    all_recommendations = []\n",
        "\n",
        "    # Process all batches\n",
        "    for batch_titles in book_titles:\n",
        "        # Generate the input for the user (including genre preferences)\n",
        "        inputs = {\n",
        "            \"user_id\": tf.expand_dims(tf.repeat(user_id, repeats=batch_titles.shape[0]), axis=0),  # User ID repeated for each book in the batch\n",
        "            \"book_title\": tf.expand_dims(batch_titles, axis=0),  # Book titles as input for the model\n",
        "            \"genre\": tf.expand_dims(tf.repeat(genre_tensor, repeats=batch_titles.shape[0], axis=0), axis=0)  # User genre preferences\n",
        "        }\n",
        "\n",
        "        # Get book recommendations for the user\n",
        "        scores = model(inputs)\n",
        "\n",
        "        # Sort the results by the generated scores\n",
        "        titles = tfr.utils.sort_by_scores(scores, [tf.expand_dims(batch_titles, axis=0)])[0]\n",
        "\n",
        "        # Store the recommendations for this batch\n",
        "        all_recommendations.extend(titles[0].numpy().tolist())  # Convert to list of titles and add to all recommendations\n",
        "\n",
        "    # Extract the top 5 recommendations from all batches\n",
        "    top_recommendations = all_recommendations[:5]\n",
        "\n",
        "    return top_recommendations"
      ],
      "metadata": {
        "id": "OV5ZXxnCd5Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage:\n",
        "# Assuming you have the model, genre_vocab, and books_df loaded already\n",
        "\n",
        "user_id = \"42\"  # Example user ID\n",
        "user_genre_preferences = ['Fiction', 'Science Fiction', 'Fantasy']  # Example genre preferences\n",
        "\n",
        "# Get book recommendations for the user\n",
        "top_recommendations = get_book_recommendations(\n",
        "    model, genre_vocab, book_details_df, user_id, user_genre_preferences\n",
        ")\n",
        "\n",
        "print(f\"Top 5 recommendations for user {user_id}: {top_recommendations}\")"
      ],
      "metadata": {
        "id": "0NXry2tz_W5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "user_id = \"42\"  # Example user ID\n",
        "user_genre_preferences = ['Fiction', 'Science Fiction', 'Fantasy']  # Example genre preferences\n",
        "\n",
        "# Get book recommendations for the user\n",
        "top_recommendations = get_book_recommendations(\n",
        "    model, genre_vocab, books_details_df, user_id, user_genre_preferences\n",
        ")\n",
        "\n",
        "print(f\"Top 5 recommendations for user {user_id}: {top_recommendations}\")"
      ],
      "metadata": {
        "id": "B5ICtr-z-1l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['genre'].unique()"
      ],
      "metadata": {
        "id": "4mjql3cih_Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "user_id = \"A30TK6U7DNS82R\"\n",
        "favorite_genres = ['religion']\n",
        "\n",
        "# Books recomendation\n",
        "recommended_books = recommend_books(\n",
        "    user_id=user_id,\n",
        "    favorite_genres=favorite_genres,\n",
        "    model=recommender_model,\n",
        "    user_vocab=user_vocab,\n",
        "    book_vocab=book_vocab,\n",
        "    genre_vocab=genre_vocab,\n",
        "    num_books=200000,\n",
        "    top_n=3\n",
        ")\n",
        "\n",
        "# Output hasil rekomendasi\n",
        "print(\"Top 3 Recommended Books:\", recommended_books)"
      ],
      "metadata": {
        "id": "PfymcWODeTEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles_to_search = ['Wuthering Heights (College classics in English)', 'The Scarlet Letter (Courage Unabridged Classics)']\n",
        "filtered_df = merged_df[merged_df['book_title'].isin(titles_to_search)][['genre', 'book_title', 'user_rating']]\n",
        "filtered_df"
      ],
      "metadata": {
        "id": "bC33nOK_f0L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3: Content-Based Filtering"
      ],
      "metadata": {
        "id": "RQ9l51DV8i0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "zwFfdL5o0vYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'review/score': 'user_rating', 'User_id': 'user_id', 'Title': 'book_title', 'categories': 'genre'})"
      ],
      "metadata": {
        "id": "ySc8hxTjnJ_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ga6pPpEKhdXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['genre'].value_counts()"
      ],
      "metadata": {
        "id": "VY5VO7MmmzZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df = df.groupby('book_title').agg(\n",
        "    average_rating=('user_rating', 'mean'),  # Calculate average of ratings\n",
        "    rating_count=('user_rating', 'size')     # Count the number of ratings (i.e., number of entries)\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "rsxhtWk2_0o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df.head()"
      ],
      "metadata": {
        "id": "QhN1HTvuBSvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = summary_df[summary_df.duplicated()]\n",
        "print(duplicate_rows)"
      ],
      "metadata": {
        "id": "YGIh1mVHty6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unique = df[[\"book_title\", \"description\", \"authors\", \"genre\", \"publisher\", \"Price\", \"image\", \"previewLink\", \"infoLink\"]].drop_duplicates()\n",
        "df_unique.head()"
      ],
      "metadata": {
        "id": "MFUzK3O-ueCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_df = df_unique.merge(summary_df, on='book_title', how='left')"
      ],
      "metadata": {
        "id": "z1_CVBpJBUoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_df.head()"
      ],
      "metadata": {
        "id": "FeL07K4VC3zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = join_df[join_df.duplicated()]\n",
        "print(duplicate_rows)"
      ],
      "metadata": {
        "id": "Uu_VxMEmuhBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R = join_df['average_rating']\n",
        "v = join_df['rating_count']\n",
        "# Only consider movies that have more votes than at least 80% of the movies in our dataset\n",
        "m = join_df['rating_count'].quantile(0.8)\n",
        "C = join_df['average_rating'].mean()\n",
        "\n",
        "join_df['weighted_average'] = (R*v + C*m)/(v+m)"
      ],
      "metadata": {
        "id": "vf_w3AjEC5Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(join_df[['weighted_average']])\n",
        "weighted_df = pd.DataFrame(scaled, columns=['weighted_average'])\n",
        "\n",
        "weighted_df.index = join_df['book_title']"
      ],
      "metadata": {
        "id": "Cqh0wRfSDRu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_df_sorted = weighted_df.sort_values(by='weighted_average', ascending=False)\n",
        "weighted_df_sorted.head(10)"
      ],
      "metadata": {
        "id": "gZgcXmEjHNU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc1(text):\n",
        "    # Check if the text is not NaN or a non-string value\n",
        "    if isinstance(text, str):\n",
        "        cleaned = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "        clean_text = cleaned.translate(str.maketrans('', '', string.digits))\n",
        "        return clean_text\n",
        "    return ''  # Return an empty string if the value is not a string\n",
        "def remove_punc2(text):\n",
        "    # Check if the text is a string\n",
        "    if isinstance(text, str):\n",
        "        cleaned = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "        clean_text = cleaned.translate(str.maketrans('', '', string.digits))\n",
        "        return clean_text\n",
        "    return ''  # Return an empty string if the value is not a string"
      ],
      "metadata": {
        "id": "PRDYTx1dDj0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_df = df_unique[['book_title', 'description', 'authors', 'publisher', 'genre']]"
      ],
      "metadata": {
        "id": "ExJ7ymIIEKJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# content_df['Title_Content'] = content_df['Title'].apply(remove_punc1)\n",
        "content_df['description'] = content_df['description'].apply(remove_punc1)\n",
        "content_df['authors'] = content_df['authors'].apply(remove_punc2)\n",
        "content_df['publisher'] = content_df['publisher'].apply(remove_punc1)\n",
        "content_df['genre'] = content_df['genre'].apply(remove_punc2)\n",
        "content_df['bag_of_words'] = ''\n",
        "content_df['bag_of_words'] = content_df[content_df.columns[1:]].apply(lambda x: ' '.join(x), axis=1)\n",
        "content_df.set_index('book_title', inplace=True)\n",
        "\n",
        "content_df = content_df[['bag_of_words']]\n",
        "content_df.head()"
      ],
      "metadata": {
        "id": "LrCCyMtdEjc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_df = weighted_df_sorted.merge(content_df, left_index=True, right_index=True, how='left')\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english', min_df=5)\n",
        "tfidf_matrix = tfidf.fit_transform(content_df['bag_of_words'])\n",
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "id": "H22ACOxGF_rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_sim = cosine_similarity(tfidf_matrix)\n",
        "cos_sim.shape"
      ],
      "metadata": {
        "id": "TnvyH0CmHEpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_df = content_df.reset_index()\n",
        "content_df.head()"
      ],
      "metadata": {
        "id": "tzKIMYjRzGQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_df = join_df.merge(content_df, on='book_title', how='left')\n",
        "join_df.head()"
      ],
      "metadata": {
        "id": "1D6Rn18EwJGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_df.info()"
      ],
      "metadata": {
        "id": "Xx-sJezNwo8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_df = join_df.drop(columns=['weighted_average_x'])"
      ],
      "metadata": {
        "id": "vIoJcR_oxG9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_df = join_df.rename(columns={'weighted_average_y': 'weighted_average'})"
      ],
      "metadata": {
        "id": "Y_8CsFfexRp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_df.head()"
      ],
      "metadata": {
        "id": "vayQZG8Zxegu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_df.info()"
      ],
      "metadata": {
        "id": "z-mDIIwbxp88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = join_df[join_df.duplicated()]\n",
        "duplicate_rows"
      ],
      "metadata": {
        "id": "985pZYuNtOL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_df = join_df.drop_duplicates()"
      ],
      "metadata": {
        "id": "cvyge6ROw1e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = join_df[join_df.duplicated()]\n",
        "duplicate_rows"
      ],
      "metadata": {
        "id": "qPR3M7_fxJoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "2Dvvtfal2YJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "join_df.to_csv('content_df.csv', index=False)"
      ],
      "metadata": {
        "id": "rbrSaRmjzzeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(tfidf_matrix, open('cosine_similarity.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "uuGualdF02tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "03yH5V5i0bjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display catalog-style recommendations\n",
        "def display_catalog(recommendations, top_n=3):\n",
        "    display_str = f\"<h3>Top {top_n} Recommendations:</h3><br>\"\n",
        "\n",
        "    for idx, row in recommendations.iterrows():\n",
        "        display_str += f\"\"\"\n",
        "            <div style=\"border: 1px solid #ddd; padding: 10px; margin-bottom: 10px;\">\n",
        "                <h4>{row['book_title']}</h4>\n",
        "                <p><strong>Authors:</strong> {row['authors']}</p>\n",
        "                <p><strong>Genre:</strong> {row['genre']}</p>\n",
        "                <p><strong>Publisher:</strong> {row['publisher']}</p>\n",
        "                <p><strong>Price:</strong> {row['Price']}</p>\n",
        "                <p><strong>Description:</strong> {row['description']}</p>\n",
        "                <img src=\"{row['image']}\" alt=\"{row['book_title']}\" width=\"100\" height=\"150\" style=\"display:block; margin-top: 10px;\">\n",
        "                <a href=\"{row['previewLink']}\" target=\"_blank\">Preview</a> |\n",
        "                <a href=\"{row['infoLink']}\" target=\"_blank\">More Info</a>\n",
        "            </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Display the catalog-style information\n",
        "    display(HTML(display_str))"
      ],
      "metadata": {
        "id": "ChGMtxT8x7l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(title, data, cos_sim, similarity_weight=0.7, top_n=3):\n",
        "    index_movie = data[data['book_title'] == title].index\n",
        "    similarity = cos_sim[index_movie].T\n",
        "\n",
        "    sim_df = pd.DataFrame(similarity, columns=['similarity'])\n",
        "    final_df = pd.concat([data, sim_df], axis=1)\n",
        "\n",
        "    final_df['final_score'] = final_df['weighted_average']*(1-similarity_weight) + final_df['similarity']*similarity_weight\n",
        "\n",
        "    final_df_sorted = final_df.sort_values(by='final_score', ascending=False).head(top_n)\n",
        "    final_df_sorted_show = final_df_sorted[['book_title', 'description', 'authors', 'genre', 'publisher', 'Price', 'image', 'previewLink', 'infoLink']]\n",
        "    return display_catalog(final_df_sorted_show, top_n=3)"
      ],
      "metadata": {
        "id": "TMqCDmyOHzxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the content_df from CSV\n",
        "content_df = pd.read_csv('/content/content_df.csv')\n",
        "\n",
        "# Load the cosine similarity matrix from pickle\n",
        "cos_sim = pickle.load(open('/content/cosine_similarity.pkl', 'rb'))\n",
        "# Convert the cosine similarity matrix to a dense format\n",
        "cos_sim_dense = cos_sim.toarray()"
      ],
      "metadata": {
        "id": "m2MenVPw1Nbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dropdown widget for book titles\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=content_df['book_title'].tolist(),\n",
        "    description='Book Title:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Function to display top 5 recommendations when a book is selected\n",
        "def on_select(change):\n",
        "    selected_title = change.new\n",
        "    recommendations_html = predict(selected_title, content_df, cos_sim_dense, similarity_weight=0.7, top_n=5)\n",
        "\n",
        "    # Display the catalog-style information\n",
        "    display(HTML(recommendations_html))\n",
        "\n",
        "# Attach the function to the dropdown widget\n",
        "dropdown.observe(on_select, names='value')\n",
        "\n",
        "# Display the dropdown widget\n",
        "display(dropdown)"
      ],
      "metadata": {
        "id": "Q0v8aBRvFtbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4: Text Classification"
      ],
      "metadata": {
        "id": "r3_DXZVa5v9E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6ohz31pXtgR"
      },
      "source": [
        "## Preparing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'review/score': 'user_rating', 'User_id': 'user_id', 'Title': 'book_title', 'categories': 'genre'})"
      ],
      "metadata": {
        "id": "EtGwQ7DoXtgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['book_title', 'authors', 'genre']].head()"
      ],
      "metadata": {
        "id": "9mCTFeDEXtgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_books_df = df[['book_title', 'authors', 'genre']].drop_duplicates()\n",
        "unique_books_df.head()"
      ],
      "metadata": {
        "id": "NEOM4iQPXtgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_books_df)"
      ],
      "metadata": {
        "id": "ROwI2fXvYF1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_books_df['title_author'] = unique_books_df['book_title'] + \" \" + unique_books_df['authors']"
      ],
      "metadata": {
        "id": "EJyp0zTFHn6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_books_df = unique_books_df.dropna(subset=['title_author'])"
      ],
      "metadata": {
        "id": "-pRtpXPkav1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_books_df)"
      ],
      "metadata": {
        "id": "uBNyOgVLa4tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Instantiate the LabelEncoder\n",
        "label_encoder = LabelEncoder()"
      ],
      "metadata": {
        "id": "0f3I-dfwge2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_books_df['genre_encoded'] = label_encoder.fit_transform(unique_books_df['genre'])"
      ],
      "metadata": {
        "id": "1ZGVcZMIgZUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the label_encoder to a file\n",
        "with open('label_encoder.pkl', 'wb') as file:\n",
        "    pickle.dump(label_encoder, file)"
      ],
      "metadata": {
        "id": "eW-AZA6i06Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_books_df.head()"
      ],
      "metadata": {
        "id": "az41wYCAYQqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_books_df['genre_encoded'].value_counts()"
      ],
      "metadata": {
        "id": "hm2ok3jXgjQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paramaters"
      ],
      "metadata": {
        "id": "hNUzTM9N1Sce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of examples to use for training\n",
        "TRAINING_SIZE = 20000\n",
        "\n",
        "# Vocabulary size of the tokenizer\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Maximum length of the padded sequences\n",
        "MAX_LENGTH = 32\n",
        "\n",
        "# Type of padding\n",
        "PADDING_TYPE = 'pre'\n",
        "\n",
        "# Specifies how to truncate the sequences\n",
        "TRUNC_TYPE = 'post'"
      ],
      "metadata": {
        "id": "dFVRu65wYyhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Dataset"
      ],
      "metadata": {
        "id": "MYDg4Vn41fxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(sentence):\n",
        "    # List of stopwords\n",
        "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "    # Sentence converted to lowercase-only\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Get all the comma separated words in a list\n",
        "    word_list = sentence.split()\n",
        "\n",
        "    # Keep all the words which are not stopwords\n",
        "    words = [w for w in word_list if w not in stopwords]\n",
        "\n",
        "    # Reconstruct sentence after discarding all stopwords\n",
        "    sentence = \" \".join(words)\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "UEOOrxtiZAoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(\n",
        "    unique_books_df[[\"title_author\", \"genre_encoded\"]],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=unique_books_df[\"genre_encoded\"]  # Ensure proportional split based on 'genre'\n",
        ")\n",
        "\n",
        "# Check the distribution of genres\n",
        "print(\"Training set genre distribution:\\n\", train_df[\"genre_encoded\"].value_counts(normalize=True))\n",
        "print(\"\\nTesting set genre distribution:\\n\", test_df[\"genre_encoded\"].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "OwWioxrg5z-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_df['title_author'].apply(remove_stopwords).values\n",
        "train_labels = train_df['genre_encoded'].values\n",
        "\n",
        "test_sentences = test_df['title_author'].apply(remove_stopwords).values\n",
        "test_labels = test_df['genre_encoded'].values"
      ],
      "metadata": {
        "id": "mRv3pbUkZwBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "C4NeT5HN1svv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the vectorization layer\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "\n",
        "# Generate the vocabulary based on the training inputs\n",
        "vectorize_layer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "01n5ukXcdsgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the vocabulary to a file\n",
        "vocabulary = vectorize_layer.get_vocabulary()\n",
        "\n",
        "# Save the vocabulary using pickle\n",
        "with open('vectorizer_vocab.pkl', 'wb') as file:\n",
        "    pickle.dump(vocabulary, file)"
      ],
      "metadata": {
        "id": "IOapXezi-j_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the sentences and labels in a tf.data.Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences,train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences,test_labels))"
      ],
      "metadata": {
        "id": "jSAroSm7dzfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_fn(dataset):\n",
        "  '''Generates padded sequences from a tf.data.Dataset'''\n",
        "\n",
        "  # Apply the vectorization layer to the string features\n",
        "  dataset_sequences = dataset.map(\n",
        "      lambda text, label: (vectorize_layer(text), label)\n",
        "      )\n",
        "\n",
        "  # Put all elements in a single ragged batch\n",
        "  dataset_sequences = dataset_sequences.ragged_batch(\n",
        "      batch_size=dataset_sequences.cardinality()\n",
        "      )\n",
        "\n",
        "  # Output a tensor from the single batch. Extract the sequences and labels.\n",
        "  sequences, labels = dataset_sequences.get_single_element()\n",
        "\n",
        "  # Pad the sequences\n",
        "  padded_sequences = tf.keras.utils.pad_sequences(\n",
        "      sequences.numpy(),\n",
        "      maxlen=MAX_LENGTH,\n",
        "      truncating=TRUNC_TYPE,\n",
        "      padding=PADDING_TYPE\n",
        "      )\n",
        "\n",
        "  # Convert back to a tf.data.Dataset\n",
        "  padded_sequences = tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
        "  labels = tf.data.Dataset.from_tensor_slices(labels)\n",
        "\n",
        "  # Combine the padded sequences and labels\n",
        "  dataset_vectorized = tf.data.Dataset.zip(padded_sequences, labels)\n",
        "\n",
        "  return dataset_vectorized"
      ],
      "metadata": {
        "id": "aPTzg8S0d5cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the train and test data\n",
        "train_dataset_vectorized = train_dataset.apply(preprocessing_fn)\n",
        "test_dataset_vectorized = test_dataset.apply(preprocessing_fn)"
      ],
      "metadata": {
        "id": "UtllGxnqd6eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View 2 training sequences and its labels\n",
        "for example in train_dataset_vectorized.take(2):\n",
        "  print(example)\n",
        "  print()"
      ],
      "metadata": {
        "id": "stRJ2bjbd-Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Optimize and batch the datasets for training\n",
        "train_dataset_final = (train_dataset_vectorized\n",
        "                       .cache()\n",
        "                       .shuffle(SHUFFLE_BUFFER_SIZE)\n",
        "                       .prefetch(PREFETCH_BUFFER_SIZE)\n",
        "                       .batch(BATCH_SIZE)\n",
        "                       )\n",
        "\n",
        "test_dataset_final = (test_dataset_vectorized\n",
        "                      .cache()\n",
        "                      .prefetch(PREFETCH_BUFFER_SIZE)\n",
        "                      .batch(BATCH_SIZE)\n",
        "                      )"
      ],
      "metadata": {
        "id": "UChr5jhCeDDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Utility"
      ],
      "metadata": {
        "id": "1Ql1kVou12Wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_acc(history):\n",
        "  '''Plots the training and validation loss and accuracy from a history object'''\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
        "  ax[0].plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "  ax[0].plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "  ax[0].set_title('Training and validation accuracy')\n",
        "  ax[0].set_xlabel('epochs')\n",
        "  ax[0].set_ylabel('accuracy')\n",
        "  ax[0].legend()\n",
        "\n",
        "  ax[1].plot(epochs, loss, 'bo', label='Training Loss')\n",
        "  ax[1].plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "  ax[1].set_title('Training and validation loss')\n",
        "  ax[1].set_xlabel('epochs')\n",
        "  ax[1].set_ylabel('loss')\n",
        "  ax[1].legend()\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "m8_ahtvMeXhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and Compile the Model"
      ],
      "metadata": {
        "id": "c8fyTHWb185t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "EMBEDDING_DIM = 16\n",
        "LSTM_DIM = 32\n",
        "DENSE_DIM = 24\n",
        "\n",
        "# Model definition with LSTM\n",
        "model_lstm = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(MAX_LENGTH,)),\n",
        "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_DIM)),\n",
        "    tf.keras.layers.Dense(DENSE_DIM, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Set the training parameters\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "c4eSscLqea8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "rihl8p4c2J7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Train the model\n",
        "history_lstm = model_lstm.fit(train_dataset_final, epochs=NUM_EPOCHS, validation_data=test_dataset_final)"
      ],
      "metadata": {
        "id": "rGuiHqczel7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy and loss\n",
        "plot_loss_acc(history_lstm)"
      ],
      "metadata": {
        "id": "uTGNTRPclTTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "Bm7WLc5P2OlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model weights\n",
        "model_lstm.save_weights('model_genre_classification_weights.h5')"
      ],
      "metadata": {
        "id": "5RBdUC4RlwSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "hxxutHiS2Qi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model weights (this assumes the model is already defined)\n",
        "model_lstm.load_weights('/content/model_genre_classification_weights.h5')"
      ],
      "metadata": {
        "id": "n7G0bL3L2UtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text preprocessing and prediction function\n",
        "def predict_genre(text):\n",
        "    # Preprocess the text input (remove stopwords and vectorize)\n",
        "    processed_text = remove_stopwords(text)  # Assuming 'remove_stopwords' is defined\n",
        "\n",
        "    # Apply the text vectorization\n",
        "    vectorized_text = vectorize_layer([processed_text])  # Apply vectorization\n",
        "\n",
        "    # Pad the vectorized input to ensure it's of the correct length (MAX_LENGTH)\n",
        "    padded_text = tf.keras.preprocessing.sequence.pad_sequences(vectorized_text, maxlen=MAX_LENGTH, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
        "\n",
        "    # Predict the genre probabilities\n",
        "    genre_probabilities = model_lstm.predict(padded_text)\n",
        "\n",
        "    # Get the predicted genre index (class with the highest probability)\n",
        "    predicted_genre_index = np.argmax(genre_probabilities, axis=1)[0]\n",
        "\n",
        "    # Map the predicted index to the genre name using the inverse of the LabelEncoder\n",
        "    predicted_genre = label_encoder.inverse_transform([predicted_genre_index])[0]\n",
        "\n",
        "    return predicted_genre"
      ],
      "metadata": {
        "id": "pGSz3kNFbjgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('label_encoder.pkl', 'rb') as file:\n",
        "    label_encoder = pickle.load(file)"
      ],
      "metadata": {
        "id": "qDuJIyx51GUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"I'll Be Seeing You\"\n",
        "predicted_genre = predict_genre(input_text)\n",
        "print(f'The predicted genre for the \"{input_text}\" text is: \"{predicted_genre}\"')"
      ],
      "metadata": {
        "id": "fGCMEQFuly6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 5: OCR"
      ],
      "metadata": {
        "id": "wFYA3yj4bdTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_image(image_path):\n",
        "    try:\n",
        "        # Open the image\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "        # Extract text using Tesseract\n",
        "        extracted_text = pytesseract.image_to_string(img)\n",
        "\n",
        "        # Clean the extracted text\n",
        "        preprocessed_text = ' '.join(extracted_text.split())\n",
        "\n",
        "        # Display the extracted text\n",
        "        print(\"Extracted text from the image:\")\n",
        "        print(preprocessed_text)\n",
        "\n",
        "        # Ask the user if the result is correct\n",
        "        is_correct = input(\"\\nIs the extracted text correct? (y/n): \").strip().lower()\n",
        "\n",
        "        # If incorrect, allow manual input\n",
        "        if is_correct == 'n':\n",
        "            preprocessed_text = input(\"Please enter the text manually: \").strip()\n",
        "\n",
        "        # Return the final text\n",
        "        return preprocessed_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "Dx4IIwqomW2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "image_path = '/content/book-covers-big-2019101610.jpg'\n",
        "result_text = extract_text_from_image(image_path)\n",
        "print(\"\\nFinal processed text:\")\n",
        "print(result_text)"
      ],
      "metadata": {
        "id": "ZlzJflOldCTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OjMJ_OtDlIJA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}